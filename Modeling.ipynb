{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558eb1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_predict, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "\n",
    "# Set the path to find my dataprocessing functions\n",
    "import sys\n",
    "sys.path.append('./preprocessing')\n",
    "\n",
    "# Load custom libraries\n",
    "from vectorization import load_vectorizer, load_sparse_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91b5eb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/NLP_cleaned.csv', index_col= 'ClaimNumber')\n",
    "\n",
    "tfidf = load_vectorizer('tfidf_vectorizer.pkl')\n",
    "X_tfidf = load_sparse_matrix('X_tfidf.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cee96369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target and drop from features\n",
    "y = df['UltimateIncurredClaimCost'].values\n",
    "X = df.drop(columns=['UltimateIncurredClaimCost', 'DateTimeOfAccident', 'DateReported', 'ClaimDescription'], axis=1)\n",
    "\n",
    "# Automatically select numeric and categorical columns by dtype\n",
    "numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Extract numeric data and scale\n",
    "X_numeric = X[numeric_cols].values\n",
    "scaler = StandardScaler()\n",
    "X_numeric_scaled = scaler.fit_transform(X_numeric)\n",
    "\n",
    "# One-hot encode categorical data, dropping first to avoid multicollinearity\n",
    "ohe = OneHotEncoder(drop='first', sparse_output=True)\n",
    "X_categorical = ohe.fit_transform(X[categorical_cols])\n",
    "\n",
    "# Convert numeric and categorical to sparse matrices to combine with TF-IDF sparse matrix\n",
    "X_numeric_sparse = sp.csr_matrix(X_numeric_scaled)\n",
    "X_categorical_sparse = sp.csr_matrix(X_categorical)\n",
    "\n",
    "# Stack numeric and categorical features horizontally\n",
    "X_other = sp.hstack([X_numeric_sparse, X_categorical_sparse])\n",
    "\n",
    "# Combine engineered features (TF-IDF) with other features\n",
    "X_final = sp.hstack([X_other, X_tfidf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ca0a26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/ohe.pkl']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save scaler and one hot encoder for fitting test data\n",
    "\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "scaler.fit(X_numeric)\n",
    "ohe.fit(X[categorical_cols])\n",
    "\n",
    "joblib.dump(scaler, 'models/scaler.pkl')\n",
    "joblib.dump(ohe, 'models/ohe.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0873d31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Model          RMSE        R2    Adj_R2           MAE\n",
      "0            OLS (Train)  46229.363079  0.257566  0.249210  15847.394490\n",
      "1  OLS (Validation - CV)  46798.159758  0.239184  0.230621  16122.215659\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['models/ols_model.pkl']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit OLS model\n",
    "\n",
    "# Define adjusted R2 function\n",
    "def adjusted_r2(r2, n, p):\n",
    "    return 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "\n",
    "# Initialize the OLS model\n",
    "lr_model = LinearRegression()\n",
    "\n",
    "# Fit the model training data\n",
    "lr_model.fit(X_final, y)\n",
    "\n",
    "# Generate predictions on the training data\n",
    "y_train_pred = lr_model.predict(X_final)\n",
    "\n",
    "# Generate cross-validated predictions\n",
    "y_val_pred = cross_val_predict(lr_model, X_final, y, cv=5, n_jobs=-1)\n",
    "\n",
    "# Function to collect performance metrics\n",
    "def get_metrics(y_true, y_pred, label):\n",
    "    n = len(y_true)\n",
    "    p = X_final.shape[1]\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return {\n",
    "        'Model': label,\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "        'R2': r2,\n",
    "        'Adj_R2': adjusted_r2(r2, n, p),\n",
    "        'MAE': mean_absolute_error(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "# Collect performance metrics\n",
    "train_metrics = get_metrics(y, y_train_pred, 'OLS (Train)')\n",
    "val_metrics = get_metrics(y, y_val_pred, 'OLS (Validation - CV)')\n",
    "\n",
    "# Combine results\n",
    "results_df = pd.DataFrame([train_metrics, val_metrics])\n",
    "print(results_df)\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(lr_model, 'models/ols_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f3bbf8",
   "metadata": {},
   "source": [
    "The next code block is fitting random forest without regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb442c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['models/rf_best_params.pkl']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform gridsearch for random forest hyperparameters without regularization\n",
    "\n",
    "# Only a 20% randomized sample of the data was used here due to compute/time constraints\n",
    "sample_frac = 0.2\n",
    "df_sample = df.sample(frac=sample_frac, random_state=49)\n",
    "\n",
    "# Convert sample index labels to integer row positions for sparse matrix slicing\n",
    "row_positions = df.index.get_indexer(df_sample.index)\n",
    "\n",
    "# Subset the combined sparse feature matrix using integer row positions\n",
    "X_sample = X_final[row_positions, :]\n",
    "\n",
    "# Subset target variable using .iloc with integer positions\n",
    "y_sample = df.iloc[row_positions]['UltimateIncurredClaimCost']\n",
    "\n",
    "# Define pipeline with Random Forest Regressor\n",
    "pipeline = Pipeline([\n",
    "    ('rf', RandomForestRegressor(random_state=49))\n",
    "])\n",
    "\n",
    "# Parameter grid for GridSearch\n",
    "param_grid = {\n",
    "    'rf__n_estimators': [100, 200],\n",
    "    'rf__max_depth': [10, 20, None],\n",
    "    'rf__min_samples_split': [2, 5],\n",
    "    'rf__min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    scoring='neg_mean_squared_error'\n",
    ")\n",
    "\n",
    "# Fit GridSearch only on the sampled subset\n",
    "grid_search.fit(X_sample, y_sample)\n",
    "\n",
    "# Get the best pipeline\n",
    "best_pipeline = grid_search.best_estimator_\n",
    "\n",
    "# Extract the optimized hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Save the hyperparameters\n",
    "joblib.dump(best_params, 'models/rf_best_params.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee1240a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             Model          RMSE        R2    Adj_R2  \\\n",
      "0            Random Forest (Train)  40153.286901  0.439902  0.433598   \n",
      "1  Random Forest (Validation - CV)  46156.829378  0.259894  0.251564   \n",
      "\n",
      "            MAE  \n",
      "0  13497.862632  \n",
      "1  14425.021733  \n"
     ]
    }
   ],
   "source": [
    "# Fit the random forest model using optimized hyperparameters\n",
    "\n",
    "# Load the best parameters from grid search\n",
    "joblib.load('models/rf_best_params.pkl')\n",
    "\n",
    "# Adjusted R^2 function\n",
    "def adjusted_r2(r2, n, p):\n",
    "    return 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "\n",
    "# Initialize model\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=best_params['rf__n_estimators'],\n",
    "    max_depth=best_params['rf__max_depth'],\n",
    "    min_samples_split=best_params['rf__min_samples_split'],\n",
    "    min_samples_leaf=best_params['rf__min_samples_leaf'],\n",
    "    random_state=49,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the model to the full training data\n",
    "rf.fit(X_final, y)\n",
    "\n",
    "# Predict on train data\n",
    "y_train_pred = rf.predict(X_final)\n",
    "\n",
    "# predict on cross validated model\n",
    "y_val_pred = cross_val_predict(rf, X_final, y, cv=5, n_jobs=-1)\n",
    "\n",
    "# Metrics helper\n",
    "def get_metrics(y_true, y_pred, label):\n",
    "    n = len(y_true)\n",
    "    p = X_final.shape[1]\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return {\n",
    "        'Model': label,\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "        'R2': r2,\n",
    "        'Adj_R2': adjusted_r2(r2, n, p),\n",
    "        'MAE': mean_absolute_error(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "# Collect both sets of metrics\n",
    "train_metrics = get_metrics(y, y_train_pred, 'Random Forest (Train)')\n",
    "val_metrics = get_metrics(y, y_val_pred, 'Random Forest (Validation - CV)')\n",
    "\n",
    "# Combine metrics\n",
    "results_df = pd.DataFrame([train_metrics, val_metrics])\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e7b304",
   "metadata": {},
   "source": [
    "This next code block is fitting GBM without regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb8d1bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['models/gbm_best_params.pkl']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Search for optimal GBM hyperparameters\n",
    "\n",
    "# Define pipeline with GBM\n",
    "pipeline = Pipeline([\n",
    "    ('gbm', GradientBoostingRegressor(random_state=49))\n",
    "])\n",
    "\n",
    "# Parameter grid for GridSearch\n",
    "param_grid = {\n",
    "    'gbm__n_estimators': [100],\n",
    "    'gbm__learning_rate': [0.1, 0.05, 0.01],\n",
    "    'gbm__max_depth': [3, 5, 7],\n",
    "    'gbm__min_samples_split': [2, 5],\n",
    "    'gbm__min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    scoring='neg_mean_squared_error'\n",
    ")\n",
    "\n",
    "# Fit GridSearch on the dataset\n",
    "grid_search.fit(X_final, df['UltimateIncurredClaimCost'])\n",
    "\n",
    "# Retrieve the best hyperparameters\n",
    "best_pipeline = grid_search.best_estimator_\n",
    "\n",
    "# Save the hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "joblib.dump(best_params, 'models/gbm_best_params.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6808dbe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Model          RMSE        R2           MAE\n",
      "0            GBM (Train)  43494.936300  0.342797  14000.988140\n",
      "1  GBM (Validation - CV)  45616.544656  0.277119  14330.583302\n"
     ]
    }
   ],
   "source": [
    "# Fit the GBM model with optimized hyperparameters\n",
    "\n",
    "# Load best parameters from grid search\n",
    "best_params = joblib.load('models/gbm_best_params.pkl')\n",
    "\n",
    "# Initialize GBM model using the best parameters from grid search\n",
    "gbm = GradientBoostingRegressor(\n",
    "    n_estimators=best_params['gbm__n_estimators'],\n",
    "    learning_rate=best_params['gbm__learning_rate'],\n",
    "    max_depth=best_params['gbm__max_depth'],\n",
    "    min_samples_split=best_params.get('gbm__min_samples_split', 2),  # Defaulting to 2 if not found\n",
    "    min_samples_leaf=best_params.get('gbm__min_samples_leaf', 1),  # Defaulting to 1 if not found\n",
    "    random_state=49\n",
    ")\n",
    "\n",
    "# Fit the model to the full training data\n",
    "gbm.fit(X_final, y)\n",
    "\n",
    "# --- Predict on training data ---\n",
    "y_train_pred = gbm.predict(X_final)\n",
    "\n",
    "# --- Cross-validated predictions (validation performance) ---\n",
    "y_val_pred = cross_val_predict(gbm, X_final, y, cv=5, n_jobs=-1)\n",
    "\n",
    "# Metrics helper function\n",
    "def get_metrics(y_true, y_pred, label):\n",
    "    n = len(y_true)\n",
    "    p = X_final.shape[1]\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return {\n",
    "        'Model': label,\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "        'R2': r2,\n",
    "        'MAE': mean_absolute_error(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "# Collect both sets of metrics\n",
    "train_metrics = get_metrics(y, y_train_pred, 'GBM (Train)')\n",
    "val_metrics = get_metrics(y, y_val_pred, 'GBM (Validation - CV)')\n",
    "\n",
    "# Combine results into a DataFrame for comparison\n",
    "results_df = pd.DataFrame([train_metrics, val_metrics])\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f53f40f",
   "metadata": {},
   "source": [
    "This next code section is fitting using lightgbm without regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330d2cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['models/lgbm_best_params.pkl']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Search for optimal LightGBM hyperparameters\n",
    "\n",
    "X_train = X_final\n",
    "y_train = df['UltimateIncurredClaimCost']\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('lgbm', lgb.LGBMRegressor(random_state=49, verbose=-1))\n",
    "])\n",
    "\n",
    "# Adjusted parameter grid for LightGBM\n",
    "param_grid = {\n",
    "    'lgbm__n_estimators': [100, 200],\n",
    "    'lgbm__learning_rate': [0.1, 0.05, 0.01],\n",
    "    'lgbm__max_depth': [3, 5, 7],\n",
    "    'lgbm__num_leaves': [31, 50],  # LightGBM-specific, affects tree complexity\n",
    "    'lgbm__min_child_samples': [5, 10]  # Equivalent to `min_samples_leaf` in GBM/XGBoost\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV for LightGBM\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    scoring='neg_mean_squared_error'\n",
    ")\n",
    "\n",
    "# Fit GridSearch on data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Retrieve the best hyperparameters\n",
    "best_pipeline = grid_search.best_estimator_\n",
    "\n",
    "# Save the hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "joblib.dump(best_params, 'models/lgbm_best_params.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae873788",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brien\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Model          RMSE        R2           MAE\n",
      "0            LightGBM (Train)  43983.732816  0.327943  13954.890971\n",
      "1  LightGBM (Validation - CV)  45441.055999  0.282670  14254.671916\n"
     ]
    }
   ],
   "source": [
    "# Fit LightGBM with optimized hyperparameters\n",
    "\n",
    "# Load best parameters from grid search\n",
    "best_params = joblib.load('models/lgbm_best_params.pkl')\n",
    "\n",
    "# Initialize LightGBM model\n",
    "lgbm = lgb.LGBMRegressor(\n",
    "    n_estimators=best_params['lgbm__n_estimators'],\n",
    "    learning_rate=best_params['lgbm__learning_rate'],\n",
    "    max_depth=best_params['lgbm__max_depth'],\n",
    "    num_leaves=best_params.get('lgbm__num_leaves', 31),  # Defaulting to 31 if not found\n",
    "    min_child_samples=best_params.get('lgbm__min_child_samples', 10),  # Defaulting to 10 if not found\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "# Fit the model to the full training data\n",
    "lgbm.fit(X_final, y)\n",
    "\n",
    "# Predict on training data\n",
    "y_train_pred = lgbm.predict(X_final)\n",
    "\n",
    "# Cross-validated predictions\n",
    "y_val_pred = cross_val_predict(lgbm, X_final, y, cv=5, n_jobs=-1)\n",
    "\n",
    "# Metrics helper function\n",
    "def get_metrics(y_true, y_pred, label):\n",
    "    n = len(y_true)\n",
    "    p = X_final.shape[1]\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return {\n",
    "        'Model': label,\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "        'R2': r2,\n",
    "        'MAE': mean_absolute_error(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "# Collect both sets of metrics\n",
    "train_metrics = get_metrics(y, y_train_pred, 'LightGBM (Train)')\n",
    "val_metrics = get_metrics(y, y_val_pred, 'LightGBM (Validation - CV)')\n",
    "\n",
    "# Combine results\n",
    "results_df = pd.DataFrame([train_metrics, val_metrics])\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81556694",
   "metadata": {},
   "source": [
    "This next code block is fitting xgboost without regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7767ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['models/xgb_best_params.pkl']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Search for optimal XGBoost hyperparameters\n",
    "\n",
    "X_train = X_final\n",
    "y_train = df['UltimateIncurredClaimCost']\n",
    "\n",
    "# Define pipeline with XGBoost Regressor\n",
    "pipeline = Pipeline([\n",
    "    ('xgb', \n",
    "    xgb.XGBRegressor(objective='reg:squarederror', \n",
    "    random_state=49, \n",
    "    verbosity=1, \n",
    "    tree_method='hist'))\n",
    "])\n",
    "\n",
    "# Adjusted parameter grid for XGBoost\n",
    "param_grid = {\n",
    "    'xgb__n_estimators': [100, 200],\n",
    "    'xgb__learning_rate': [0.1, 0.05, 0.01],\n",
    "    'xgb__max_depth': [3, 5, 7],\n",
    "    'xgb__subsample': [0.8, 1.0],  # Fraction of training samples used per boosting round\n",
    "    'xgb__colsample_bytree': [0.8, 1.0],  # Fraction of features used per tree\n",
    "    'xgb__min_child_weight': [1, 5]  # Equivalent to min_samples_leaf in LightGBM\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV for XGBoost\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    scoring='neg_mean_squared_error'\n",
    ")\n",
    "\n",
    "# Fit GridSearch on entire dataset\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Retrieve the best pipeline (with optimized hyperparameters)\n",
    "best_pipeline = grid_search.best_estimator_\n",
    "\n",
    "# Save the best parameters dictionary to a file\n",
    "best_params = grid_search.best_params_\n",
    "joblib.dump(best_params, 'models/xgb_best_params.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5890efd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Model          RMSE        R2           MAE\n",
      "0            XGBoost (Train)  44240.764768  0.320065  14072.723872\n",
      "1  XGBoost (Validation - CV)  45520.518022  0.280159  14303.856369\n"
     ]
    }
   ],
   "source": [
    "# Fit XGBoost model with optimized hyperparameters\n",
    "\n",
    "# Load best parameters from grid search\n",
    "best_params = joblib.load('models/xgb_best_params.pkl')\n",
    "\n",
    "# Initialize XGBoost model\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=best_params['xgb__n_estimators'],\n",
    "    learning_rate=best_params['xgb__learning_rate'],\n",
    "    max_depth=best_params['xgb__max_depth'],\n",
    "    subsample=best_params.get('xgb__subsample', 1.0),  # Defaulting to 1.0 if not found\n",
    "    colsample_bytree=best_params.get('xgb__colsample_bytree', 1.0),  # Defaulting to 1.0 if not found\n",
    "    min_child_weight=best_params.get('xgb__min_child_weight', 1),  # Defaulting to 1 if not found\n",
    "    objective='reg:squarederror',\n",
    "    random_state=49,\n",
    "    verbosity=1,\n",
    "    tree_method='hist'  # Optimized for speed on large datasets\n",
    ")\n",
    "\n",
    "# Fit the model to the full training data\n",
    "xgb_model.fit(X_final, y)\n",
    "\n",
    "# Predict on training data\n",
    "y_train_pred = xgb_model.predict(X_final)\n",
    "\n",
    "# Cross-validated predictions\n",
    "y_val_pred = cross_val_predict(xgb_model, X_final, y, cv=5, n_jobs=-1)\n",
    "\n",
    "# Metrics helper function\n",
    "def get_metrics(y_true, y_pred, label):\n",
    "    n = len(y_true)\n",
    "    p = X_final.shape[1]\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return {\n",
    "        'Model': label,\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "        'R2': r2,\n",
    "        'MAE': mean_absolute_error(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "# Collect both sets of metrics\n",
    "train_metrics = get_metrics(y, y_train_pred, 'XGBoost (Train)')\n",
    "val_metrics = get_metrics(y, y_val_pred, 'XGBoost (Validation - CV)')\n",
    "\n",
    "# Combine results into a DataFrame for comparison\n",
    "results_df = pd.DataFrame([train_metrics, val_metrics])\n",
    "print(results_df)\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(xgb_model, 'models/xgb_trained_model.pkl')\n",
    "\n",
    "# Load it later for inference\n",
    "xgb_model = joblib.load('models/xgb_trained_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92d0d79",
   "metadata": {},
   "source": [
    "This next code block implements l1 and l2 regularization with xgboost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4111ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1296 candidates, totalling 6480 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['models/xgbR_best_params.pkl']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Search for optimal hyperparameters with regularization\n",
    "\n",
    "# Feature matrix and target\n",
    "X_train = X_final\n",
    "y_train = df['UltimateIncurredClaimCost']\n",
    "\n",
    "# Define pipeline with XGBoost Regressor\n",
    "pipeline = Pipeline([\n",
    "    ('xgb', xgb.XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        random_state=49,\n",
    "        verbosity=1,\n",
    "        tree_method='hist'\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Parameter grid including regularization terms\n",
    "param_grid = {\n",
    "    'xgb__n_estimators': [100, 200],\n",
    "    'xgb__learning_rate': [0.1, 0.05, 0.01],\n",
    "    'xgb__max_depth': [3, 5, 7],\n",
    "    'xgb__subsample': [0.8, 1.0],\n",
    "    'xgb__colsample_bytree': [0.8, 1.0],\n",
    "    'xgb__min_child_weight': [1, 5],\n",
    "    'xgb__reg_alpha': [0, 0.1, 1],      # L1 regularization (sparsity)\n",
    "    'xgb__reg_lambda': [1, 5, 10]       # L2 regularization (shrinkage)\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV for XGBoost\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    scoring='neg_mean_squared_error'\n",
    ")\n",
    "\n",
    "# Fit GridSearch on training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Retrieve best pipeline and best parameters\n",
    "best_pipeline = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Save the best parameters with a new filename\n",
    "joblib.dump(best_params, 'models/xgbR_best_params.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99b76bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Model          RMSE       R2           MAE\n",
      "0            XGBoost (Train)  44131.814040  0.32341  13948.964519\n",
      "1  XGBoost (Validation - CV)  45376.394168  0.28471  14197.748994\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['models/xgbR_trained_model.pkl']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the XGBoost model with optimized hyperparameters\n",
    "\n",
    "# Load the best parameters from the grid search with regularization\n",
    "best_params = joblib.load('models/xgbR_best_params.pkl')\n",
    "\n",
    "# Initialize XGBoost model\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=best_params['xgb__n_estimators'],\n",
    "    learning_rate=best_params['xgb__learning_rate'],\n",
    "    max_depth=best_params['xgb__max_depth'],\n",
    "    subsample=best_params.get('xgb__subsample', 1.0),\n",
    "    colsample_bytree=best_params.get('xgb__colsample_bytree', 1.0),\n",
    "    min_child_weight=best_params.get('xgb__min_child_weight', 1),\n",
    "    reg_alpha=best_params.get('xgb__reg_alpha', 0),\n",
    "    reg_lambda=best_params.get('xgb__reg_lambda', 1),\n",
    "    objective='reg:squarederror',\n",
    "    random_state=49,\n",
    "    verbosity=1,\n",
    "    tree_method='hist'\n",
    ")\n",
    "\n",
    "# Fit the model to the full training data\n",
    "xgb_model.fit(X_final, y)\n",
    "\n",
    "# Predict on training data\n",
    "y_train_pred = xgb_model.predict(X_final)\n",
    "\n",
    "# Cross-validated predictions\n",
    "y_val_pred = cross_val_predict(xgb_model, X_final, y, cv=5, n_jobs=-1)\n",
    "\n",
    "# Metrics helper function\n",
    "def get_metrics(y_true, y_pred, label):\n",
    "    return {\n",
    "        'Model': label,\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "        'R2': r2_score(y_true, y_pred),\n",
    "        'MAE': mean_absolute_error(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "# Collect both sets of metrics\n",
    "train_metrics = get_metrics(y, y_train_pred, 'XGBoost (Train)')\n",
    "val_metrics = get_metrics(y, y_val_pred, 'XGBoost (Validation - CV)')\n",
    "\n",
    "# Combine results\n",
    "results_df = pd.DataFrame([train_metrics, val_metrics])\n",
    "print(results_df)\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(xgb_model, 'models/xgbR_trained_model.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65c703b",
   "metadata": {},
   "source": [
    "This next code block uses bayesian hyperparemeter search with regularization on lightgbm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b2a047",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-06 02:46:48,713] A new study created in memory with name: no-name-9333b0f9-ed96-4a4f-ac7e-6c4d0c9e20d3\n",
      "C:\\Users\\brien\\AppData\\Local\\Temp\\ipykernel_7932\\2103451473.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
      "C:\\Users\\brien\\AppData\\Local\\Temp\\ipykernel_7932\\2103451473.py:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-8, 1.0),\n",
      "C:\\Users\\brien\\AppData\\Local\\Temp\\ipykernel_7932\\2103451473.py:21: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-8, 1.0),\n",
      "C:\\Users\\brien\\AppData\\Local\\Temp\\ipykernel_7932\\2103451473.py:22: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'min_split_gain': trial.suggest_uniform('min_split_gain', 0.0, 0.1),\n",
      "[I 2025-06-06 02:46:49,856] Trial 0 finished with value: 45475.92588854774 and parameters: {'n_estimators': 100, 'learning_rate': 0.047012592040529357, 'max_depth': 3, 'num_leaves': 40, 'min_child_samples': 8, 'reg_alpha': 0.011601366312535182, 'reg_lambda': 8.922736719833639e-06, 'min_split_gain': 0.007793969525605338}. Best is trial 0 with value: 45475.92588854774.\n",
      "[I 2025-06-06 02:46:51,843] Trial 1 finished with value: 45944.739914458805 and parameters: {'n_estimators': 100, 'learning_rate': 0.028986865620810216, 'max_depth': 6, 'num_leaves': 37, 'min_child_samples': 7, 'reg_alpha': 0.003913629114261187, 'reg_lambda': 3.487834355832766e-06, 'min_split_gain': 0.02443543590453037}. Best is trial 0 with value: 45475.92588854774.\n",
      "[I 2025-06-06 02:46:53,817] Trial 2 finished with value: 45941.93030847314 and parameters: {'n_estimators': 100, 'learning_rate': 0.056292770672585375, 'max_depth': 6, 'num_leaves': 45, 'min_child_samples': 9, 'reg_alpha': 0.3897813605711221, 'reg_lambda': 7.099540602624541e-07, 'min_split_gain': 0.0326050042828108}. Best is trial 0 with value: 45475.92588854774.\n",
      "[I 2025-06-06 02:46:55,150] Trial 3 finished with value: 46529.605481874365 and parameters: {'n_estimators': 100, 'learning_rate': 0.011697700918523209, 'max_depth': 4, 'num_leaves': 47, 'min_child_samples': 6, 'reg_alpha': 1.820236979072084e-05, 'reg_lambda': 1.0441578977342752e-08, 'min_split_gain': 0.05544709623231958}. Best is trial 0 with value: 45475.92588854774.\n",
      "[I 2025-06-06 02:46:56,863] Trial 4 finished with value: 45611.71737023677 and parameters: {'n_estimators': 200, 'learning_rate': 0.01393975277688832, 'max_depth': 3, 'num_leaves': 40, 'min_child_samples': 7, 'reg_alpha': 0.005668079927082126, 'reg_lambda': 0.006286423392297276, 'min_split_gain': 0.029330353729004877}. Best is trial 0 with value: 45475.92588854774.\n",
      "[I 2025-06-06 02:46:57,927] Trial 5 finished with value: 45660.54543746933 and parameters: {'n_estimators': 100, 'learning_rate': 0.09367230813982882, 'max_depth': 4, 'num_leaves': 47, 'min_child_samples': 6, 'reg_alpha': 6.525314411992232e-08, 'reg_lambda': 7.900695279856608e-08, 'min_split_gain': 0.09700995717793727}. Best is trial 0 with value: 45475.92588854774.\n",
      "[I 2025-06-06 02:46:58,889] Trial 6 finished with value: 45491.18391564647 and parameters: {'n_estimators': 100, 'learning_rate': 0.05309090765846827, 'max_depth': 3, 'num_leaves': 49, 'min_child_samples': 5, 'reg_alpha': 1.6026741008312305e-06, 'reg_lambda': 0.34227281342042987, 'min_split_gain': 0.022834274744774688}. Best is trial 0 with value: 45475.92588854774.\n",
      "[I 2025-06-06 02:47:00,286] Trial 7 finished with value: 45532.80364972248 and parameters: {'n_estimators': 100, 'learning_rate': 0.056018002437821256, 'max_depth': 4, 'num_leaves': 48, 'min_child_samples': 8, 'reg_alpha': 0.01941604145557523, 'reg_lambda': 0.45109903883519165, 'min_split_gain': 0.033580788531244736}. Best is trial 0 with value: 45475.92588854774.\n",
      "[I 2025-06-06 02:47:01,708] Trial 8 finished with value: 45744.075943208685 and parameters: {'n_estimators': 100, 'learning_rate': 0.042831546900857366, 'max_depth': 5, 'num_leaves': 36, 'min_child_samples': 9, 'reg_alpha': 3.9876877284867594e-08, 'reg_lambda': 0.0047536628566405845, 'min_split_gain': 0.05727253053627712}. Best is trial 0 with value: 45475.92588854774.\n",
      "[I 2025-06-06 02:47:03,712] Trial 9 finished with value: 46033.4152346433 and parameters: {'n_estimators': 100, 'learning_rate': 0.051117123946114625, 'max_depth': 7, 'num_leaves': 43, 'min_child_samples': 5, 'reg_alpha': 2.9307609983843424e-05, 'reg_lambda': 8.514859786768029e-08, 'min_split_gain': 0.03451965050260847}. Best is trial 0 with value: 45475.92588854774.\n",
      "[I 2025-06-06 02:47:05,182] Trial 10 finished with value: 45443.24397941779 and parameters: {'n_estimators': 200, 'learning_rate': 0.02444688102485692, 'max_depth': 3, 'num_leaves': 31, 'min_child_samples': 10, 'reg_alpha': 0.4438784730736187, 'reg_lambda': 4.5724482685422086e-05, 'min_split_gain': 0.003531056812357182}. Best is trial 10 with value: 45443.24397941779.\n",
      "[I 2025-06-06 02:47:06,664] Trial 11 finished with value: 45471.649666659934 and parameters: {'n_estimators': 200, 'learning_rate': 0.02083225892061959, 'max_depth': 3, 'num_leaves': 31, 'min_child_samples': 10, 'reg_alpha': 0.5078116497569575, 'reg_lambda': 5.9287539295114727e-05, 'min_split_gain': 0.001687529516414499}. Best is trial 10 with value: 45443.24397941779.\n",
      "[I 2025-06-06 02:47:08,216] Trial 12 finished with value: 45489.238542485655 and parameters: {'n_estimators': 200, 'learning_rate': 0.019079332979996727, 'max_depth': 3, 'num_leaves': 31, 'min_child_samples': 10, 'reg_alpha': 0.6714954304399834, 'reg_lambda': 0.00015895578578217258, 'min_split_gain': 0.0003957533015763945}. Best is trial 10 with value: 45443.24397941779.\n",
      "[I 2025-06-06 02:47:10,156] Trial 13 finished with value: 45546.285388699696 and parameters: {'n_estimators': 200, 'learning_rate': 0.022899609344185716, 'max_depth': 4, 'num_leaves': 31, 'min_child_samples': 10, 'reg_alpha': 0.12318763581366592, 'reg_lambda': 0.00014013083149440255, 'min_split_gain': 0.010195991071281676}. Best is trial 10 with value: 45443.24397941779.\n",
      "[I 2025-06-06 02:47:12,772] Trial 14 finished with value: 45740.48609827481 and parameters: {'n_estimators': 200, 'learning_rate': 0.01697792004193009, 'max_depth': 5, 'num_leaves': 34, 'min_child_samples': 10, 'reg_alpha': 0.001300457502907534, 'reg_lambda': 0.0032380147537496655, 'min_split_gain': 0.07333612053404613}. Best is trial 10 with value: 45443.24397941779.\n",
      "[I 2025-06-06 02:47:14,204] Trial 15 finished with value: 45458.04681294645 and parameters: {'n_estimators': 200, 'learning_rate': 0.0314717571212432, 'max_depth': 3, 'num_leaves': 34, 'min_child_samples': 9, 'reg_alpha': 0.00042415217283538724, 'reg_lambda': 2.2413586592945784e-05, 'min_split_gain': 0.014337456542489465}. Best is trial 10 with value: 45443.24397941779.\n",
      "[I 2025-06-06 02:47:16,518] Trial 16 finished with value: 45769.556442887406 and parameters: {'n_estimators': 200, 'learning_rate': 0.03194812971813068, 'max_depth': 5, 'num_leaves': 34, 'min_child_samples': 9, 'reg_alpha': 0.00026999610820775666, 'reg_lambda': 1.921968199069756e-05, 'min_split_gain': 0.015340248715699879}. Best is trial 10 with value: 45443.24397941779.\n",
      "[I 2025-06-06 02:47:18,291] Trial 17 finished with value: 45553.86009644383 and parameters: {'n_estimators': 200, 'learning_rate': 0.03139921972258976, 'max_depth': 4, 'num_leaves': 35, 'min_child_samples': 9, 'reg_alpha': 1.2342459772068414e-06, 'reg_lambda': 0.0008993671072842335, 'min_split_gain': 0.042725879515114}. Best is trial 10 with value: 45443.24397941779.\n",
      "[I 2025-06-06 02:47:20,564] Trial 18 finished with value: 46052.828691604846 and parameters: {'n_estimators': 200, 'learning_rate': 0.07732501189663642, 'max_depth': 6, 'num_leaves': 37, 'min_child_samples': 8, 'reg_alpha': 0.0765318037584277, 'reg_lambda': 1.3440553269380612e-06, 'min_split_gain': 0.0707118145107847}. Best is trial 10 with value: 45443.24397941779.\n",
      "[I 2025-06-06 02:47:22,021] Trial 19 finished with value: 45452.80706141891 and parameters: {'n_estimators': 200, 'learning_rate': 0.027113262832549834, 'max_depth': 3, 'num_leaves': 33, 'min_child_samples': 9, 'reg_alpha': 0.0003433435761147314, 'reg_lambda': 0.042989763445525055, 'min_split_gain': 0.01733251754214645}. Best is trial 10 with value: 45443.24397941779.\n",
      "[I 2025-06-06 02:47:24,207] Trial 20 finished with value: 45753.74878150389 and parameters: {'n_estimators': 200, 'learning_rate': 0.01024600735113813, 'max_depth': 4, 'num_leaves': 38, 'min_child_samples': 10, 'reg_alpha': 2.9294892157765505e-06, 'reg_lambda': 0.044539681971426034, 'min_split_gain': 0.04693020685957906}. Best is trial 10 with value: 45443.24397941779.\n",
      "[I 2025-06-06 02:47:25,611] Trial 21 finished with value: 45458.42860635946 and parameters: {'n_estimators': 200, 'learning_rate': 0.026589133052132816, 'max_depth': 3, 'num_leaves': 33, 'min_child_samples': 9, 'reg_alpha': 0.0004348167244479872, 'reg_lambda': 0.00047553568280551614, 'min_split_gain': 0.016521542436936754}. Best is trial 10 with value: 45443.24397941779.\n",
      "[I 2025-06-06 02:47:26,986] Trial 22 finished with value: 45457.239435669 and parameters: {'n_estimators': 200, 'learning_rate': 0.037091591631228354, 'max_depth': 3, 'num_leaves': 33, 'min_child_samples': 9, 'reg_alpha': 8.039989168312937e-05, 'reg_lambda': 0.02928763954982206, 'min_split_gain': 0.017875521147294477}. Best is trial 10 with value: 45443.24397941779.\n",
      "[I 2025-06-06 02:47:28,388] Trial 23 finished with value: 45467.300225602696 and parameters: {'n_estimators': 200, 'learning_rate': 0.03868216988168386, 'max_depth': 3, 'num_leaves': 32, 'min_child_samples': 8, 'reg_alpha': 4.674586247533758e-05, 'reg_lambda': 0.08227739812173364, 'min_split_gain': 0.02049882805638094}. Best is trial 10 with value: 45443.24397941779.\n",
      "[I 2025-06-06 02:47:30,269] Trial 24 finished with value: 45534.564805585804 and parameters: {'n_estimators': 200, 'learning_rate': 0.025127838018591676, 'max_depth': 4, 'num_leaves': 33, 'min_child_samples': 10, 'reg_alpha': 6.655674983844064e-06, 'reg_lambda': 0.038003299180400955, 'min_split_gain': 0.004560395281344141}. Best is trial 10 with value: 45443.24397941779.\n",
      "[I 2025-06-06 02:47:31,805] Trial 25 finished with value: 45543.28882270509 and parameters: {'n_estimators': 200, 'learning_rate': 0.016348947997082412, 'max_depth': 3, 'num_leaves': 42, 'min_child_samples': 9, 'reg_alpha': 2.4080677445905416e-07, 'reg_lambda': 0.0012500536131293356, 'min_split_gain': 0.009678754750706273}. Best is trial 10 with value: 45443.24397941779.\n",
      "[I 2025-06-06 02:47:33,869] Trial 26 finished with value: 45771.71551787945 and parameters: {'n_estimators': 200, 'learning_rate': 0.04117755383733211, 'max_depth': 5, 'num_leaves': 38, 'min_child_samples': 8, 'reg_alpha': 0.00011371501253355776, 'reg_lambda': 0.012780549019434597, 'min_split_gain': 0.04088477039733955}. Best is trial 10 with value: 45443.24397941779.\n",
      "[I 2025-06-06 02:47:37,201] Trial 27 finished with value: 45924.83230689291 and parameters: {'n_estimators': 200, 'learning_rate': 0.03615909310302135, 'max_depth': 7, 'num_leaves': 33, 'min_child_samples': 10, 'reg_alpha': 3.9321798213529817e-07, 'reg_lambda': 0.13978450918681815, 'min_split_gain': 0.023281689195631238}. Best is trial 10 with value: 45443.24397941779.\n",
      "[I 2025-06-06 02:47:39,289] Trial 28 finished with value: 45583.70867956407 and parameters: {'n_estimators': 200, 'learning_rate': 0.02215172447625767, 'max_depth': 4, 'num_leaves': 35, 'min_child_samples': 7, 'reg_alpha': 0.0015512078588771426, 'reg_lambda': 0.7648184846874952, 'min_split_gain': 0.012300249639907668}. Best is trial 10 with value: 45443.24397941779.\n",
      "[I 2025-06-06 02:47:40,752] Trial 29 finished with value: 45454.9300477287 and parameters: {'n_estimators': 200, 'learning_rate': 0.03522242184600513, 'max_depth': 3, 'num_leaves': 39, 'min_child_samples': 9, 'reg_alpha': 0.03287518553185527, 'reg_lambda': 0.014052331047954057, 'min_split_gain': 0.007701878992676367}. Best is trial 10 with value: 45443.24397941779.\n",
      "[I 2025-06-06 02:47:42,045] Trial 30 finished with value: 45519.15244671971 and parameters: {'n_estimators': 200, 'learning_rate': 0.06594805822258881, 'max_depth': 3, 'num_leaves': 42, 'min_child_samples': 8, 'reg_alpha': 0.06881877129043672, 'reg_lambda': 0.0003641440584703619, 'min_split_gain': 0.00629300475833283}. Best is trial 10 with value: 45443.24397941779.\n",
      "[I 2025-06-06 02:47:43,460] Trial 31 finished with value: 45461.08222503274 and parameters: {'n_estimators': 200, 'learning_rate': 0.03620040236944986, 'max_depth': 3, 'num_leaves': 39, 'min_child_samples': 9, 'reg_alpha': 0.015859053669702777, 'reg_lambda': 0.017869901017245244, 'min_split_gain': 0.0008811661377877067}. Best is trial 10 with value: 45443.24397941779.\n",
      "[I 2025-06-06 02:47:44,952] Trial 32 finished with value: 45460.77936820427 and parameters: {'n_estimators': 200, 'learning_rate': 0.02755377894610658, 'max_depth': 3, 'num_leaves': 32, 'min_child_samples': 9, 'reg_alpha': 0.027196590755353717, 'reg_lambda': 0.1439627938080658, 'min_split_gain': 0.027072961475801034}. Best is trial 10 with value: 45443.24397941779.\n",
      "[I 2025-06-06 02:47:46,327] Trial 33 finished with value: 45435.94448720661 and parameters: {'n_estimators': 200, 'learning_rate': 0.045913431725392966, 'max_depth': 3, 'num_leaves': 35, 'min_child_samples': 10, 'reg_alpha': 0.18954011597422168, 'reg_lambda': 0.0016187842290736477, 'min_split_gain': 0.019212037348565688}. Best is trial 33 with value: 45435.94448720661.\n",
      "[I 2025-06-06 02:47:48,045] Trial 34 finished with value: 45573.63716351278 and parameters: {'n_estimators': 200, 'learning_rate': 0.04791699004892175, 'max_depth': 4, 'num_leaves': 36, 'min_child_samples': 10, 'reg_alpha': 0.18127962447226542, 'reg_lambda': 0.0018230437824418965, 'min_split_gain': 0.011205874339952562}. Best is trial 33 with value: 45435.94448720661.\n",
      "[I 2025-06-06 02:47:49,282] Trial 35 finished with value: 45445.11813709585 and parameters: {'n_estimators': 200, 'learning_rate': 0.06840274861591188, 'max_depth': 3, 'num_leaves': 36, 'min_child_samples': 10, 'reg_alpha': 0.8385678659335921, 'reg_lambda': 0.01031612491059692, 'min_split_gain': 0.0069664831649761064}. Best is trial 33 with value: 45435.94448720661.\n",
      "[I 2025-06-06 02:47:51,653] Trial 36 finished with value: 45973.36182010016 and parameters: {'n_estimators': 200, 'learning_rate': 0.06654496802931527, 'max_depth': 6, 'num_leaves': 36, 'min_child_samples': 10, 'reg_alpha': 0.2927251394006912, 'reg_lambda': 2.4381820259856136e-06, 'min_split_gain': 0.029627392246583815}. Best is trial 33 with value: 45435.94448720661.\n",
      "[I 2025-06-06 02:47:52,894] Trial 37 finished with value: 45548.85724004862 and parameters: {'n_estimators': 200, 'learning_rate': 0.09938747743027992, 'max_depth': 3, 'num_leaves': 35, 'min_child_samples': 10, 'reg_alpha': 0.9476867248093515, 'reg_lambda': 0.006561338681164219, 'min_split_gain': 0.02088946502792982}. Best is trial 33 with value: 45435.94448720661.\n",
      "[I 2025-06-06 02:47:54,038] Trial 38 finished with value: 45526.79764719497 and parameters: {'n_estimators': 100, 'learning_rate': 0.06304109070138429, 'max_depth': 4, 'num_leaves': 32, 'min_child_samples': 10, 'reg_alpha': 0.005664668674217329, 'reg_lambda': 9.278906992940217e-06, 'min_split_gain': 0.09707013344893717}. Best is trial 33 with value: 45435.94448720661.\n",
      "[I 2025-06-06 02:47:55,382] Trial 39 finished with value: 45424.673868885126 and parameters: {'n_estimators': 200, 'learning_rate': 0.0433014086471699, 'max_depth': 3, 'num_leaves': 37, 'min_child_samples': 10, 'reg_alpha': 0.32221836424415634, 'reg_lambda': 5.0979670429023394e-05, 'min_split_gain': 0.0371548979725619}. Best is trial 39 with value: 45424.673868885126.\n",
      "[I 2025-06-06 02:47:56,470] Trial 40 finished with value: 45620.30667441368 and parameters: {'n_estimators': 100, 'learning_rate': 0.08110692992882178, 'max_depth': 4, 'num_leaves': 37, 'min_child_samples': 6, 'reg_alpha': 0.2130768262565193, 'reg_lambda': 7.403937909502242e-05, 'min_split_gain': 0.0542829224184479}. Best is trial 39 with value: 45424.673868885126.\n",
      "[I 2025-06-06 02:47:57,853] Trial 41 finished with value: 45427.80919201343 and parameters: {'n_estimators': 200, 'learning_rate': 0.044287086326253625, 'max_depth': 3, 'num_leaves': 35, 'min_child_samples': 10, 'reg_alpha': 0.05332618140301626, 'reg_lambda': 0.0002977002578083753, 'min_split_gain': 0.03728648867095687}. Best is trial 39 with value: 45424.673868885126.\n",
      "[I 2025-06-06 02:47:59,200] Trial 42 finished with value: 45437.16414355977 and parameters: {'n_estimators': 200, 'learning_rate': 0.04481962571851874, 'max_depth': 3, 'num_leaves': 40, 'min_child_samples': 10, 'reg_alpha': 0.9265759190985278, 'reg_lambda': 0.0003192042559411393, 'min_split_gain': 0.039490597237981144}. Best is trial 39 with value: 45424.673868885126.\n",
      "[I 2025-06-06 02:48:00,550] Trial 43 finished with value: 45429.632919628566 and parameters: {'n_estimators': 200, 'learning_rate': 0.0449556985277685, 'max_depth': 3, 'num_leaves': 41, 'min_child_samples': 10, 'reg_alpha': 0.054746504781039304, 'reg_lambda': 0.00029606440686388613, 'min_split_gain': 0.03767902760676742}. Best is trial 39 with value: 45424.673868885126.\n",
      "[I 2025-06-06 02:48:01,947] Trial 44 finished with value: 45438.78998146455 and parameters: {'n_estimators': 200, 'learning_rate': 0.04634711231901468, 'max_depth': 3, 'num_leaves': 41, 'min_child_samples': 10, 'reg_alpha': 0.05075524458613905, 'reg_lambda': 0.00024423478008850884, 'min_split_gain': 0.03780904476424647}. Best is trial 39 with value: 45424.673868885126.\n",
      "[I 2025-06-06 02:48:02,872] Trial 45 finished with value: 45440.619987953 and parameters: {'n_estimators': 100, 'learning_rate': 0.05488745008217358, 'max_depth': 3, 'num_leaves': 45, 'min_child_samples': 10, 'reg_alpha': 0.009730572379134128, 'reg_lambda': 0.0006335735930447839, 'min_split_gain': 0.048836023797968656}. Best is trial 39 with value: 45424.673868885126.\n",
      "[I 2025-06-06 02:48:04,623] Trial 46 finished with value: 45566.48869889664 and parameters: {'n_estimators': 200, 'learning_rate': 0.046161190612844655, 'max_depth': 4, 'num_leaves': 40, 'min_child_samples': 10, 'reg_alpha': 0.0029095731683487103, 'reg_lambda': 4.0541423467862146e-05, 'min_split_gain': 0.062441687817669005}. Best is trial 39 with value: 45424.673868885126.\n",
      "[I 2025-06-06 02:48:05,965] Trial 47 finished with value: 45448.458494111415 and parameters: {'n_estimators': 200, 'learning_rate': 0.040863177257180396, 'max_depth': 3, 'num_leaves': 43, 'min_child_samples': 6, 'reg_alpha': 0.16591141354948863, 'reg_lambda': 5.839882056197392e-06, 'min_split_gain': 0.03538673482977262}. Best is trial 39 with value: 45424.673868885126.\n",
      "[I 2025-06-06 02:48:06,942] Trial 48 finished with value: 45442.17434061081 and parameters: {'n_estimators': 100, 'learning_rate': 0.06049505489773619, 'max_depth': 3, 'num_leaves': 39, 'min_child_samples': 10, 'reg_alpha': 0.0912097305590869, 'reg_lambda': 0.0023741684709434525, 'min_split_gain': 0.0291037466914345}. Best is trial 39 with value: 45424.673868885126.\n",
      "[I 2025-06-06 02:48:08,588] Trial 49 finished with value: 45555.04068562214 and parameters: {'n_estimators': 200, 'learning_rate': 0.0511206057799523, 'max_depth': 4, 'num_leaves': 45, 'min_child_samples': 10, 'reg_alpha': 0.3608343005455515, 'reg_lambda': 2.6854881118497514e-07, 'min_split_gain': 0.039479379165447595}. Best is trial 39 with value: 45424.673868885126.\n",
      "[I 2025-06-06 02:48:10,171] Trial 50 finished with value: 45465.11211086355 and parameters: {'n_estimators': 200, 'learning_rate': 0.04382930960358261, 'max_depth': 3, 'num_leaves': 38, 'min_child_samples': 7, 'reg_alpha': 0.00877418427951716, 'reg_lambda': 1.467072492521426e-05, 'min_split_gain': 0.04367795755355778}. Best is trial 39 with value: 45424.673868885126.\n",
      "[I 2025-06-06 02:48:11,740] Trial 51 finished with value: 45452.735499935705 and parameters: {'n_estimators': 200, 'learning_rate': 0.047754861101579095, 'max_depth': 3, 'num_leaves': 41, 'min_child_samples': 10, 'reg_alpha': 0.04857865997828039, 'reg_lambda': 0.0002152779625920542, 'min_split_gain': 0.037266942905786235}. Best is trial 39 with value: 45424.673868885126.\n",
      "[I 2025-06-06 02:48:13,065] Trial 52 finished with value: 45455.29103412697 and parameters: {'n_estimators': 200, 'learning_rate': 0.057329677917192005, 'max_depth': 3, 'num_leaves': 41, 'min_child_samples': 10, 'reg_alpha': 1.4720604493070986e-08, 'reg_lambda': 0.0002455678848455614, 'min_split_gain': 0.051714091053849956}. Best is trial 39 with value: 45424.673868885126.\n",
      "[I 2025-06-06 02:48:14,411] Trial 53 finished with value: 45430.21558986948 and parameters: {'n_estimators': 200, 'learning_rate': 0.04446624159736089, 'max_depth': 3, 'num_leaves': 44, 'min_child_samples': 10, 'reg_alpha': 0.04417863024813033, 'reg_lambda': 9.944972519019231e-05, 'min_split_gain': 0.03171221184060726}. Best is trial 39 with value: 45424.673868885126.\n",
      "[I 2025-06-06 02:48:15,820] Trial 54 finished with value: 45460.01195415478 and parameters: {'n_estimators': 200, 'learning_rate': 0.03304243348718727, 'max_depth': 3, 'num_leaves': 46, 'min_child_samples': 9, 'reg_alpha': 0.3814273084592933, 'reg_lambda': 0.00010421637598849608, 'min_split_gain': 0.030336881744779474}. Best is trial 39 with value: 45424.673868885126.\n",
      "[I 2025-06-06 02:48:17,183] Trial 55 finished with value: 45440.971082275384 and parameters: {'n_estimators': 200, 'learning_rate': 0.05111617533369077, 'max_depth': 3, 'num_leaves': 50, 'min_child_samples': 10, 'reg_alpha': 0.11575406709755606, 'reg_lambda': 3.665236847762179e-05, 'min_split_gain': 0.0325904416540585}. Best is trial 39 with value: 45424.673868885126.\n",
      "[I 2025-06-06 02:48:18,556] Trial 56 finished with value: 45429.26357949065 and parameters: {'n_estimators': 200, 'learning_rate': 0.03998369854201195, 'max_depth': 3, 'num_leaves': 43, 'min_child_samples': 10, 'reg_alpha': 0.026541345447126566, 'reg_lambda': 0.0009706369842166534, 'min_split_gain': 0.04528177353045363}. Best is trial 39 with value: 45424.673868885126.\n",
      "[I 2025-06-06 02:48:20,311] Trial 57 finished with value: 45571.30490978446 and parameters: {'n_estimators': 200, 'learning_rate': 0.038407340872497114, 'max_depth': 4, 'num_leaves': 44, 'min_child_samples': 9, 'reg_alpha': 0.02462622700232169, 'reg_lambda': 0.0009008299483933356, 'min_split_gain': 0.0445036325178417}. Best is trial 39 with value: 45424.673868885126.\n",
      "[I 2025-06-06 02:48:22,459] Trial 58 finished with value: 45773.9992995957 and parameters: {'n_estimators': 200, 'learning_rate': 0.04069581085123549, 'max_depth': 5, 'num_leaves': 43, 'min_child_samples': 10, 'reg_alpha': 0.0031579533507539222, 'reg_lambda': 0.004530908252454697, 'min_split_gain': 0.033355235023492094}. Best is trial 39 with value: 45424.673868885126.\n",
      "[I 2025-06-06 02:48:23,417] Trial 59 finished with value: 45537.3873078013 and parameters: {'n_estimators': 100, 'learning_rate': 0.03281268500297316, 'max_depth': 3, 'num_leaves': 44, 'min_child_samples': 9, 'reg_alpha': 0.015382284149682238, 'reg_lambda': 0.00012773130500367966, 'min_split_gain': 0.058832056300283576}. Best is trial 39 with value: 45424.673868885126.\n",
      "[I 2025-06-06 02:48:24,774] Trial 60 finished with value: 45488.84056061647 and parameters: {'n_estimators': 200, 'learning_rate': 0.0297060738328016, 'max_depth': 3, 'num_leaves': 47, 'min_child_samples': 5, 'reg_alpha': 0.0010595009520207076, 'reg_lambda': 0.0013807869815112986, 'min_split_gain': 0.025465126137702594}. Best is trial 39 with value: 45424.673868885126.\n",
      "[I 2025-06-06 02:48:26,188] Trial 61 finished with value: 45439.78525508062 and parameters: {'n_estimators': 200, 'learning_rate': 0.04345579409131476, 'max_depth': 3, 'num_leaves': 44, 'min_child_samples': 10, 'reg_alpha': 0.45196496638282235, 'reg_lambda': 0.0004617381983593922, 'min_split_gain': 0.09053192801207283}. Best is trial 39 with value: 45424.673868885126.\n",
      "[I 2025-06-06 02:48:27,478] Trial 62 finished with value: 45442.91455103788 and parameters: {'n_estimators': 200, 'learning_rate': 0.05062129561484071, 'max_depth': 3, 'num_leaves': 42, 'min_child_samples': 10, 'reg_alpha': 0.13942056801862057, 'reg_lambda': 7.22217741098234e-05, 'min_split_gain': 0.04618620626878912}. Best is trial 39 with value: 45424.673868885126.\n",
      "[I 2025-06-06 02:48:28,853] Trial 63 finished with value: 45442.09817883299 and parameters: {'n_estimators': 200, 'learning_rate': 0.04472301427423041, 'max_depth': 3, 'num_leaves': 40, 'min_child_samples': 10, 'reg_alpha': 0.053134690457629835, 'reg_lambda': 0.0008618360625484442, 'min_split_gain': 0.03995717012143903}. Best is trial 39 with value: 45424.673868885126.\n",
      "[I 2025-06-06 02:48:30,220] Trial 64 finished with value: 45442.826667694244 and parameters: {'n_estimators': 200, 'learning_rate': 0.0577990274521787, 'max_depth': 3, 'num_leaves': 37, 'min_child_samples': 10, 'reg_alpha': 0.2625882696836407, 'reg_lambda': 0.00031570267226719434, 'min_split_gain': 0.05154048117959288}. Best is trial 39 with value: 45424.673868885126.\n",
      "[I 2025-06-06 02:48:31,673] Trial 65 finished with value: 45427.594001706464 and parameters: {'n_estimators': 200, 'learning_rate': 0.03993709588260139, 'max_depth': 3, 'num_leaves': 42, 'min_child_samples': 10, 'reg_alpha': 0.6518428568090178, 'reg_lambda': 3.254212030901799e-05, 'min_split_gain': 0.034297316515991064}. Best is trial 39 with value: 45424.673868885126.\n",
      "[I 2025-06-06 02:48:33,529] Trial 66 finished with value: 45451.11260047145 and parameters: {'n_estimators': 200, 'learning_rate': 0.03444594037535811, 'max_depth': 3, 'num_leaves': 43, 'min_child_samples': 9, 'reg_alpha': 0.035645467556341945, 'reg_lambda': 1.7561493716383474e-05, 'min_split_gain': 0.026216821797337364}. Best is trial 39 with value: 45424.673868885126.\n",
      "[I 2025-06-06 02:48:36,569] Trial 67 finished with value: 45998.049462785784 and parameters: {'n_estimators': 200, 'learning_rate': 0.039546544967258325, 'max_depth': 7, 'num_leaves': 42, 'min_child_samples': 10, 'reg_alpha': 0.07788565274682184, 'reg_lambda': 0.00014396342707281092, 'min_split_gain': 0.03510879073465882}. Best is trial 39 with value: 45424.673868885126.\n",
      "[I 2025-06-06 02:48:38,376] Trial 68 finished with value: 45591.71792770485 and parameters: {'n_estimators': 200, 'learning_rate': 0.05334758552570042, 'max_depth': 4, 'num_leaves': 34, 'min_child_samples': 9, 'reg_alpha': 0.006577050547752369, 'reg_lambda': 4.5268351461123626e-05, 'min_split_gain': 0.04202175531295455}. Best is trial 39 with value: 45424.673868885126.\n",
      "[I 2025-06-06 02:48:39,812] Trial 69 finished with value: 45434.4841098619 and parameters: {'n_estimators': 200, 'learning_rate': 0.029572811515244192, 'max_depth': 3, 'num_leaves': 35, 'min_child_samples': 10, 'reg_alpha': 0.015486752776773565, 'reg_lambda': 2.785263527236267e-05, 'min_split_gain': 0.020527241620583767}. Best is trial 39 with value: 45424.673868885126.\n",
      "[I 2025-06-06 02:48:42,603] Trial 70 finished with value: 45911.74767551566 and parameters: {'n_estimators': 200, 'learning_rate': 0.037139421268905896, 'max_depth': 6, 'num_leaves': 46, 'min_child_samples': 10, 'reg_alpha': 0.029013439318180418, 'reg_lambda': 3.910116719914561e-06, 'min_split_gain': 0.023209798684833468}. Best is trial 39 with value: 45424.673868885126.\n",
      "[I 2025-06-06 02:48:44,008] Trial 71 finished with value: 45428.411092804636 and parameters: {'n_estimators': 200, 'learning_rate': 0.03056792345975996, 'max_depth': 3, 'num_leaves': 34, 'min_child_samples': 10, 'reg_alpha': 0.015572352357355683, 'reg_lambda': 2.6309489859134628e-05, 'min_split_gain': 0.0193643782287632}. Best is trial 39 with value: 45424.673868885126.\n",
      "[I 2025-06-06 02:48:45,386] Trial 72 finished with value: 45434.80618376758 and parameters: {'n_estimators': 200, 'learning_rate': 0.03097024987929525, 'max_depth': 3, 'num_leaves': 35, 'min_child_samples': 10, 'reg_alpha': 0.014247759366806513, 'reg_lambda': 2.314544955724144e-05, 'min_split_gain': 0.036681266968141765}. Best is trial 39 with value: 45424.673868885126.\n",
      "[I 2025-06-06 02:48:46,883] Trial 73 finished with value: 45443.94618554953 and parameters: {'n_estimators': 200, 'learning_rate': 0.025430066409850387, 'max_depth': 3, 'num_leaves': 34, 'min_child_samples': 10, 'reg_alpha': 0.019052119709410245, 'reg_lambda': 9.655541438505177e-06, 'min_split_gain': 0.03096455068644998}. Best is trial 39 with value: 45424.673868885126.\n",
      "[I 2025-06-06 02:48:48,305] Trial 74 finished with value: 45435.69620236947 and parameters: {'n_estimators': 200, 'learning_rate': 0.028732073581088275, 'max_depth': 3, 'num_leaves': 43, 'min_child_samples': 10, 'reg_alpha': 0.10197829363894936, 'reg_lambda': 2.849222531505243e-05, 'min_split_gain': 0.02692427900358455}. Best is trial 39 with value: 45424.673868885126.\n",
      "[I 2025-06-06 02:48:49,812] Trial 75 finished with value: 45457.82998588906 and parameters: {'n_estimators': 200, 'learning_rate': 0.02312344694523527, 'max_depth': 3, 'num_leaves': 44, 'min_child_samples': 10, 'reg_alpha': 0.005098446308725649, 'reg_lambda': 1.6151289911746307e-08, 'min_split_gain': 0.04850379877841064}. Best is trial 39 with value: 45424.673868885126.\n",
      "[I 2025-06-06 02:48:51,189] Trial 76 finished with value: 45415.04114979008 and parameters: {'n_estimators': 200, 'learning_rate': 0.034954958253914745, 'max_depth': 3, 'num_leaves': 36, 'min_child_samples': 10, 'reg_alpha': 0.5178410567578727, 'reg_lambda': 9.493530719957567e-05, 'min_split_gain': 0.020784538852168948}. Best is trial 76 with value: 45415.04114979008.\n",
      "[I 2025-06-06 02:48:52,618] Trial 77 finished with value: 45448.84845800196 and parameters: {'n_estimators': 200, 'learning_rate': 0.03518477747641789, 'max_depth': 3, 'num_leaves': 36, 'min_child_samples': 9, 'reg_alpha': 0.5923708484657259, 'reg_lambda': 5.905693640019838e-05, 'min_split_gain': 0.04540411754333672}. Best is trial 76 with value: 45415.04114979008.\n",
      "[I 2025-06-06 02:48:53,978] Trial 78 finished with value: 45423.6744254959 and parameters: {'n_estimators': 200, 'learning_rate': 0.042010327088565884, 'max_depth': 3, 'num_leaves': 41, 'min_child_samples': 10, 'reg_alpha': 0.2789263181672201, 'reg_lambda': 0.00010026060888939174, 'min_split_gain': 0.013343874305641799}. Best is trial 76 with value: 45415.04114979008.\n",
      "[I 2025-06-06 02:48:55,000] Trial 79 finished with value: 45472.1913194172 and parameters: {'n_estimators': 100, 'learning_rate': 0.041507556708022225, 'max_depth': 3, 'num_leaves': 39, 'min_child_samples': 10, 'reg_alpha': 0.5896507222149047, 'reg_lambda': 0.0005613709497888926, 'min_split_gain': 0.013533448533485502}. Best is trial 76 with value: 45415.04114979008.\n",
      "[I 2025-06-06 02:48:56,692] Trial 80 finished with value: 45595.92111239124 and parameters: {'n_estimators': 200, 'learning_rate': 0.04901188056955577, 'max_depth': 4, 'num_leaves': 41, 'min_child_samples': 9, 'reg_alpha': 0.18790946571930195, 'reg_lambda': 1.5703106507038159e-06, 'min_split_gain': 0.016052979465250618}. Best is trial 76 with value: 45415.04114979008.\n",
      "[I 2025-06-06 02:48:58,012] Trial 81 finished with value: 45439.37275876442 and parameters: {'n_estimators': 200, 'learning_rate': 0.03844655919023721, 'max_depth': 3, 'num_leaves': 42, 'min_child_samples': 10, 'reg_alpha': 0.30224607090687905, 'reg_lambda': 8.305293324300102e-05, 'min_split_gain': 0.03327444487093923}. Best is trial 76 with value: 45415.04114979008.\n",
      "[I 2025-06-06 02:48:59,457] Trial 82 finished with value: 45426.78789807514 and parameters: {'n_estimators': 200, 'learning_rate': 0.04212498707158588, 'max_depth': 3, 'num_leaves': 37, 'min_child_samples': 10, 'reg_alpha': 0.061185423436351374, 'reg_lambda': 0.00019735230891431332, 'min_split_gain': 0.027949145657851816}. Best is trial 76 with value: 45415.04114979008.\n",
      "[I 2025-06-06 02:49:00,785] Trial 83 finished with value: 45434.433077433656 and parameters: {'n_estimators': 200, 'learning_rate': 0.034177187225697944, 'max_depth': 3, 'num_leaves': 37, 'min_child_samples': 10, 'reg_alpha': 0.08904597926854091, 'reg_lambda': 1.2945887816286673e-05, 'min_split_gain': 0.022758372477143247}. Best is trial 76 with value: 45415.04114979008.\n",
      "[I 2025-06-06 02:49:02,192] Trial 84 finished with value: 45430.95088925943 and parameters: {'n_estimators': 200, 'learning_rate': 0.03721760163651018, 'max_depth': 3, 'num_leaves': 38, 'min_child_samples': 10, 'reg_alpha': 0.6018635845831615, 'reg_lambda': 0.000156999260201665, 'min_split_gain': 0.009521725633569062}. Best is trial 76 with value: 45415.04114979008.\n",
      "[I 2025-06-06 02:49:03,612] Trial 85 finished with value: 45419.13664847625 and parameters: {'n_estimators': 200, 'learning_rate': 0.04207116474155865, 'max_depth': 3, 'num_leaves': 40, 'min_child_samples': 10, 'reg_alpha': 0.9880124209155736, 'reg_lambda': 0.00020121766839658033, 'min_split_gain': 0.028159843461348668}. Best is trial 76 with value: 45415.04114979008.\n",
      "[I 2025-06-06 02:49:04,973] Trial 86 finished with value: 45426.33314512716 and parameters: {'n_estimators': 200, 'learning_rate': 0.04130423061531303, 'max_depth': 3, 'num_leaves': 37, 'min_child_samples': 10, 'reg_alpha': 0.9338153418439276, 'reg_lambda': 0.000187053632264767, 'min_split_gain': 0.02899013288497715}. Best is trial 76 with value: 45415.04114979008.\n",
      "[I 2025-06-06 02:49:06,759] Trial 87 finished with value: 45566.00150116934 and parameters: {'n_estimators': 200, 'learning_rate': 0.0425131745921451, 'max_depth': 4, 'num_leaves': 36, 'min_child_samples': 10, 'reg_alpha': 0.9050686179155406, 'reg_lambda': 5.377373496231377e-06, 'min_split_gain': 0.018870694437027206}. Best is trial 76 with value: 45415.04114979008.\n",
      "[I 2025-06-06 02:49:08,197] Trial 88 finished with value: 45435.63078570807 and parameters: {'n_estimators': 200, 'learning_rate': 0.031750322770628905, 'max_depth': 3, 'num_leaves': 38, 'min_child_samples': 10, 'reg_alpha': 0.2701967024267364, 'reg_lambda': 5.386102351499458e-05, 'min_split_gain': 0.028075191435345176}. Best is trial 76 with value: 45415.04114979008.\n",
      "[I 2025-06-06 02:49:09,509] Trial 89 finished with value: 45468.821588127445 and parameters: {'n_estimators': 200, 'learning_rate': 0.04796451988267854, 'max_depth': 3, 'num_leaves': 37, 'min_child_samples': 9, 'reg_alpha': 0.43860997287143416, 'reg_lambda': 0.00021013007720469375, 'min_split_gain': 0.02484819766956015}. Best is trial 76 with value: 45415.04114979008.\n",
      "[I 2025-06-06 02:49:10,912] Trial 90 finished with value: 45426.632800073705 and parameters: {'n_estimators': 200, 'learning_rate': 0.04233065281837885, 'max_depth': 3, 'num_leaves': 34, 'min_child_samples': 10, 'reg_alpha': 0.1359908996213094, 'reg_lambda': 9.625679268418333e-05, 'min_split_gain': 0.023127705674851312}. Best is trial 76 with value: 45415.04114979008.\n",
      "[I 2025-06-06 02:49:12,349] Trial 91 finished with value: 45428.97552239069 and parameters: {'n_estimators': 200, 'learning_rate': 0.036504049701603127, 'max_depth': 3, 'num_leaves': 34, 'min_child_samples': 10, 'reg_alpha': 0.14777105108585792, 'reg_lambda': 0.00011530892612838474, 'min_split_gain': 0.022031767785965203}. Best is trial 76 with value: 45415.04114979008.\n",
      "[I 2025-06-06 02:49:13,697] Trial 92 finished with value: 45429.25577467383 and parameters: {'n_estimators': 200, 'learning_rate': 0.04211243505782896, 'max_depth': 3, 'num_leaves': 36, 'min_child_samples': 10, 'reg_alpha': 0.6530932349541444, 'reg_lambda': 3.125962711912928e-05, 'min_split_gain': 0.01754974848882409}. Best is trial 76 with value: 45415.04114979008.\n",
      "[I 2025-06-06 02:49:15,031] Trial 93 finished with value: 45430.80992140352 and parameters: {'n_estimators': 200, 'learning_rate': 0.039708452291580786, 'max_depth': 3, 'num_leaves': 33, 'min_child_samples': 10, 'reg_alpha': 0.22489435105486857, 'reg_lambda': 0.00019140964368735827, 'min_split_gain': 0.015262361254648782}. Best is trial 76 with value: 45415.04114979008.\n",
      "[I 2025-06-06 02:49:16,502] Trial 94 finished with value: 45420.686474981434 and parameters: {'n_estimators': 200, 'learning_rate': 0.033776805131443176, 'max_depth': 3, 'num_leaves': 35, 'min_child_samples': 10, 'reg_alpha': 0.38791878380952666, 'reg_lambda': 6.49404647490953e-05, 'min_split_gain': 0.028929518596016852}. Best is trial 76 with value: 45415.04114979008.\n",
      "[I 2025-06-06 02:49:18,033] Trial 95 finished with value: 45435.779650294 and parameters: {'n_estimators': 200, 'learning_rate': 0.03310679232463158, 'max_depth': 3, 'num_leaves': 35, 'min_child_samples': 10, 'reg_alpha': 0.30340118380337383, 'reg_lambda': 6.866809765553135e-05, 'min_split_gain': 0.02912452957741493}. Best is trial 76 with value: 45415.04114979008.\n",
      "[I 2025-06-06 02:49:19,099] Trial 96 finished with value: 45497.10171310999 and parameters: {'n_estimators': 100, 'learning_rate': 0.03799399758177659, 'max_depth': 3, 'num_leaves': 36, 'min_child_samples': 10, 'reg_alpha': 0.9370231128301185, 'reg_lambda': 0.00038550812532217024, 'min_split_gain': 0.024482627257885727}. Best is trial 76 with value: 45415.04114979008.\n",
      "[I 2025-06-06 02:49:20,382] Trial 97 finished with value: 45433.278241639666 and parameters: {'n_estimators': 200, 'learning_rate': 0.04272829260784052, 'max_depth': 3, 'num_leaves': 37, 'min_child_samples': 10, 'reg_alpha': 0.39903228622157977, 'reg_lambda': 0.0006612053773995227, 'min_split_gain': 0.031037847764629648}. Best is trial 76 with value: 45415.04114979008.\n",
      "[I 2025-06-06 02:49:22,012] Trial 98 finished with value: 45470.90943029344 and parameters: {'n_estimators': 200, 'learning_rate': 0.03542091026365254, 'max_depth': 3, 'num_leaves': 38, 'min_child_samples': 5, 'reg_alpha': 0.13835675731954047, 'reg_lambda': 4.600021141343332e-05, 'min_split_gain': 0.02698815163261236}. Best is trial 76 with value: 45415.04114979008.\n",
      "[I 2025-06-06 02:49:23,381] Trial 99 finished with value: 45484.54448002346 and parameters: {'n_estimators': 200, 'learning_rate': 0.047039830889843356, 'max_depth': 3, 'num_leaves': 40, 'min_child_samples': 7, 'reg_alpha': 0.5678111314057455, 'reg_lambda': 0.00011101975671653373, 'min_split_gain': 0.03529704710680413}. Best is trial 76 with value: 45415.04114979008.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'n_estimators': 200, 'learning_rate': 0.034954958253914745, 'max_depth': 3, 'num_leaves': 36, 'min_child_samples': 10, 'reg_alpha': 0.5178410567578727, 'reg_lambda': 9.493530719957567e-05, 'min_split_gain': 0.020784538852168948}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['models/lgbm_best_params_optuna.pkl']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bayesian hyperparameter search with regularization\n",
    "\n",
    "X_train = X_final\n",
    "y_train = df['UltimateIncurredClaimCost']\n",
    "\n",
    "def objective(trial):\n",
    "    # Define the hyperparameter search space\n",
    "    param = {\n",
    "        'n_estimators': trial.suggest_categorical('n_estimators', [100, 200]),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 7),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 31, 50),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 10),\n",
    "        # Regularization terms:\n",
    "        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-8, 1.0),\n",
    "        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-8, 1.0),\n",
    "        'min_split_gain': trial.suggest_uniform('min_split_gain', 0.0, 0.1),\n",
    "        'random_state': 49,\n",
    "        'verbose': -1\n",
    "    }\n",
    "\n",
    "    # Initialize the model\n",
    "    model = lgb.LGBMRegressor(**param)\n",
    "    \n",
    "    # Use cross-validation to evaluate performance.\n",
    "    scores = cross_val_score(model, X_train, y_train, \n",
    "                             cv=5, \n",
    "                             scoring='neg_mean_squared_error', \n",
    "                             n_jobs=-1)\n",
    "                             \n",
    "    # Return the RMS\n",
    "    rmse = np.sqrt(-scores.mean())\n",
    "    return rmse\n",
    "\n",
    "# Create a study object specifying that we want to minimize the RMSE\n",
    "study = optuna.create_study(direction='minimize')\n",
    "\n",
    "# Optimize the study by evaluating 100 trials\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Extract the best hyperparameters\n",
    "best_params = study.best_trial.params\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "\n",
    "# Save the hyperparameters\n",
    "joblib.dump(best_params, 'models/lgbm_best_params_optuna.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3718f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brien\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Model          RMSE        R2           MAE\n",
      "0            LightGBM (Train)  44272.579176  0.319087  13992.688167\n",
      "1  LightGBM (Validation - CV)  45414.984496  0.283493  14214.781986\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['models/lgbm_model.pkl']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit LightGBM with optimized hyperparameters\n",
    "\n",
    "# Load optimized hyperparameters\n",
    "best_params = joblib.load('models/lgbm_best_params_optuna.pkl')\n",
    "\n",
    "# Initialize LightGBM\n",
    "lgbm = lgb.LGBMRegressor(\n",
    "    n_estimators=best_params.get('n_estimators', 100),\n",
    "    learning_rate=best_params.get('learning_rate', 0.1),\n",
    "    max_depth=best_params.get('max_depth', 3),\n",
    "    num_leaves=best_params.get('num_leaves', 31),\n",
    "    min_child_samples=best_params.get('min_child_samples', 10),\n",
    "    reg_alpha=best_params.get('reg_alpha', 0.0),       # L1 regularization\n",
    "    reg_lambda=best_params.get('reg_lambda', 0.0),     # L2 regularization\n",
    "    min_split_gain=best_params.get('min_split_gain', 0.0),  # Minimum gain for a split\n",
    "    random_state=49,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "# Fit the model to the full training data\n",
    "lgbm.fit(X_final, y)\n",
    "\n",
    "# Predict on training data\n",
    "y_train_pred = lgbm.predict(X_final)\n",
    "\n",
    "# Cross-validated predictions\n",
    "y_val_pred = cross_val_predict(lgbm, X_final, y, cv=5, n_jobs=-1)\n",
    "\n",
    "# Metrics helper function\n",
    "def get_metrics(y_true, y_pred, label):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return {\n",
    "        'Model': label,\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "        'R2': r2,\n",
    "        'MAE': mean_absolute_error(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "# Collect both sets of metrics\n",
    "train_metrics = get_metrics(y, y_train_pred, 'LightGBM (Train)')\n",
    "val_metrics = get_metrics(y, y_val_pred, 'LightGBM (Validation - CV)')\n",
    "\n",
    "# Combine results\n",
    "results_df = pd.DataFrame([train_metrics, val_metrics])\n",
    "print(results_df)\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(lgbm, 'models/lgbm_model.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
